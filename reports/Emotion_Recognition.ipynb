{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion-Recognition.ipynb",
      "provenance": [],
      "mount_file_id": "1Ln6PqOazFoNJQgW_YMHW_jG2Gs3HJFsW",
      "authorship_tag": "ABX9TyN5r5Vg63CBw8qr1zCq/4+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INFJakZda/Emotion-Recognition/blob/master/reports/Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX5917kxRHqt",
        "colab_type": "text"
      },
      "source": [
        "# Importowanie bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhmDQwc_RTEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "080e9924-e09f-40c2-c85f-958705b4aad6"
      },
      "source": [
        "import argparse\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83G5cOreJFQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d1f9be-4fe8-4f6f-cd6c-d8bfa8908731"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqEiwUivRQhM",
        "colab_type": "text"
      },
      "source": [
        "# Funkcje pomocnicze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvsMNMSPPDmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getOneLayerLSTM(embeddingMatrix):\n",
        "    \"\"\"Constructs the architecture of the model\n",
        "    Input:\n",
        "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
        "    Output:\n",
        "        model : A basic LSTM model\n",
        "    \"\"\"\n",
        "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
        "                               EMBEDDING_DIM,\n",
        "                               weights=[embeddingMatrix],\n",
        "                               input_length=MAX_SEQUENCE_LENGTH,\n",
        "                               trainable=False)\n",
        "    model = Sequential()\n",
        "    model.add(embeddingLayer)\n",
        "    model.add(LSTM(LSTM_DIM, dropout=DROPOUT))\n",
        "    model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC_UksqOKgla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessData(dataFilePath, mode, eos=\" <eos> \"):\n",
        "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
        "    Input:\n",
        "        dataFilePath : Path to train/test file to be processed\n",
        "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
        "    Output:\n",
        "        indices : Unique conversation ID list\n",
        "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
        "        labels : [Only available in \"train\" mode] List of labels\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    conversations = []\n",
        "    labels = []\n",
        "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
        "        finput.readline()\n",
        "        for line in finput:\n",
        "            # Convert multiple instances of . ? ! , to single instance\n",
        "            # okay...sure -> okay . sure\n",
        "            # okay???sure -> okay ? sure\n",
        "            # Add whitespace around such punctuation\n",
        "            # okay!sure -> okay ! sure\n",
        "            repeatedChars = ['.', '?', '!', ',']\n",
        "            for c in repeatedChars:\n",
        "                lineSplit = line.split(c)\n",
        "                while True:\n",
        "                    try:\n",
        "                        lineSplit.remove('')\n",
        "                    except:\n",
        "                        break\n",
        "                cSpace = ' ' + c + ' '\n",
        "                line = cSpace.join(lineSplit)\n",
        "\n",
        "            line = line.strip().split('\\t')\n",
        "            if mode == \"train\":\n",
        "                # Train data contains id, 3 turns and label\n",
        "                label = emotion2label[line[4]]\n",
        "                labels.append(label)\n",
        "\n",
        "            conv = f'{eos}'.join(line[1:4])\n",
        "\n",
        "            # Remove any duplicate spaces\n",
        "            duplicateSpacePattern = re.compile(r'\\ +')\n",
        "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
        "\n",
        "            indices.append(int(line[0]))\n",
        "            conversations.append(conv.lower())\n",
        "\n",
        "    if mode == \"train\":\n",
        "        return indices, conversations, labels\n",
        "    else:\n",
        "        return indices, conversations"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olB06zRUL5v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEmbeddingMatrix(wordIndex):\n",
        "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
        "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
        "    Input:\n",
        "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
        "    Output:\n",
        "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
        "    \"\"\"\n",
        "    embeddingsIndex = {}\n",
        "    # Load the embedding vectors from ther GloVe file\n",
        "    with io.open(gloveDir, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddingsIndex[word] = embeddingVector\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
        "\n",
        "    # Minimum word index of any word is 1. \n",
        "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
        "    for word, i in wordIndex.items():\n",
        "        embeddingVector = embeddingsIndex.get(word)\n",
        "        if embeddingVector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embeddingMatrix[i] = embeddingVector\n",
        "\n",
        "    return embeddingMatrix"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNaTW9TLbpP",
        "colab_type": "text"
      },
      "source": [
        "# Kod wykonawczy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k29zUFVlRCqX",
        "colab_type": "text"
      },
      "source": [
        "## Definicja stałych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvIDBfw-J1Uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM=10000\n",
        "EMBEDDING_DIM=100\n",
        "LSTM_DIM=120\n",
        "NUM_CLASSES=4\n",
        "MAX_SEQUENCE_LENGTH=200\n",
        "DROPOUT=0.2\n",
        "LEARNING_RATE=0.003\n",
        "NUM_FOLDS=5\n",
        "MAX_NB_WORDS=20000\n",
        "BATCH_SIZE=200\n",
        "NUM_EPOCHS=100"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ3WBPefVBT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mainPath = '/content/drive/My Drive/Magisterka/'\n",
        "\n",
        "trainDataPath = mainPath + 'train.txt'\n",
        "devDataPath = mainPath + 'test.txt'\n",
        "testDataPath = mainPath + 'test.txt'\n",
        "\n",
        "solutionPath = mainPath + 'solution/'\n",
        "\n",
        "gloveDir = mainPath + 'glove.6B.100d.txt'\n",
        "\n",
        "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
        "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PaB9bkpLhbq",
        "colab_type": "text"
      },
      "source": [
        "## Przetwarzanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvrfngZYKmkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainIndices, trainTexts, trainLabels = preprocessData(trainDataPath, mode=\"train\")\n",
        "devIndices, devTexts, devLabels = preprocessData(devDataPath, mode=\"train\")\n",
        "testIndices, testTexts, testLabels = preprocessData(testDataPath, mode=\"train\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp0tBYt5KwLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(trainTexts)\n",
        "\n",
        "trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
        "devSequences = tokenizer.texts_to_sequences(devTexts)\n",
        "testSequences = tokenizer.texts_to_sequences(testTexts)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdxqJJZ2SLY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f61005be-b6c7-4279-ed91-10bc09102108"
      },
      "source": [
        "wordIndex = tokenizer.word_index\n",
        "embeddingMatrix = getEmbeddingMatrix(wordIndex)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLaju_GdSXiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTrain = pad_sequences(trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "dataDev = pad_sequences(devSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "dataTest = pad_sequences(testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "trainLabels = to_categorical(np.asarray(trainLabels))\n",
        "devLabels = to_categorical(np.asarray(devLabels))\n",
        "testLabels = to_categorical(np.asarray(testLabels))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWaYLnQLS0bk",
        "colab_type": "text"
      },
      "source": [
        "## Budowa i ewaluacja modeli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR0u6E2vTQfS",
        "colab_type": "text"
      },
      "source": [
        "### Jednowarstwowy LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqUdH1hXdOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "68ab8a53-da25-4b35-ed1c-8ccc2780d1ef"
      },
      "source": [
        "model = getOneLayerLSTM(embeddingMatrix)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 100)          1683200   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 120)               106080    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 484       \n",
            "=================================================================\n",
            "Total params: 1,789,764\n",
            "Trainable params: 106,564\n",
            "Non-trainable params: 1,683,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZFDh0dXSvgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c38742c-b26e-4d73-8e40-1b0e1da8abe0"
      },
      "source": [
        "history = model.fit(dataTrain, trainLabels, validation_data=(dataDev, devLabels), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
        "model.save(mainPath + 'EP%d_LR%de-5_LDim%d_BS%d.h5' % (NUM_EPOCHS, int(LEARNING_RATE * (10 ** 5)), LSTM_DIM, BATCH_SIZE))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "151/151 [==============================] - 10s 66ms/step - loss: 0.8590 - acc: 0.6561 - val_loss: 0.4092 - val_acc: 0.8689\n",
            "Epoch 2/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.5956 - acc: 0.7719 - val_loss: 0.3781 - val_acc: 0.8698\n",
            "Epoch 3/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.4990 - acc: 0.8118 - val_loss: 0.4711 - val_acc: 0.8230\n",
            "Epoch 4/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.4450 - acc: 0.8330 - val_loss: 0.3612 - val_acc: 0.8733\n",
            "Epoch 5/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.4083 - acc: 0.8472 - val_loss: 0.4165 - val_acc: 0.8517\n",
            "Epoch 6/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.3779 - acc: 0.8588 - val_loss: 0.3699 - val_acc: 0.8695\n",
            "Epoch 7/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.3538 - acc: 0.8664 - val_loss: 0.3761 - val_acc: 0.8648\n",
            "Epoch 8/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.3325 - acc: 0.8734 - val_loss: 0.4370 - val_acc: 0.8448\n",
            "Epoch 9/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.3281 - acc: 0.8783 - val_loss: 0.4255 - val_acc: 0.8530\n",
            "Epoch 10/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.3010 - acc: 0.8874 - val_loss: 0.3669 - val_acc: 0.8729\n",
            "Epoch 11/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.2784 - acc: 0.8965 - val_loss: 0.4477 - val_acc: 0.8481\n",
            "Epoch 12/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.2636 - acc: 0.9002 - val_loss: 0.5068 - val_acc: 0.8308\n",
            "Epoch 13/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.2498 - acc: 0.9069 - val_loss: 0.4327 - val_acc: 0.8579\n",
            "Epoch 14/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.2325 - acc: 0.9121 - val_loss: 0.4574 - val_acc: 0.8531\n",
            "Epoch 15/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.2218 - acc: 0.9179 - val_loss: 0.4784 - val_acc: 0.8450\n",
            "Epoch 16/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.2107 - acc: 0.9208 - val_loss: 0.4096 - val_acc: 0.8717\n",
            "Epoch 17/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1989 - acc: 0.9263 - val_loss: 0.4358 - val_acc: 0.8671\n",
            "Epoch 18/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1881 - acc: 0.9324 - val_loss: 0.4586 - val_acc: 0.8620\n",
            "Epoch 19/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1777 - acc: 0.9338 - val_loss: 0.4475 - val_acc: 0.8711\n",
            "Epoch 20/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1690 - acc: 0.9368 - val_loss: 0.5520 - val_acc: 0.8388\n",
            "Epoch 21/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.1726 - acc: 0.9365 - val_loss: 0.4972 - val_acc: 0.8568\n",
            "Epoch 22/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1540 - acc: 0.9431 - val_loss: 0.5073 - val_acc: 0.8581\n",
            "Epoch 23/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1485 - acc: 0.9455 - val_loss: 0.5270 - val_acc: 0.8481\n",
            "Epoch 24/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1444 - acc: 0.9463 - val_loss: 0.5403 - val_acc: 0.8486\n",
            "Epoch 25/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1368 - acc: 0.9497 - val_loss: 0.5519 - val_acc: 0.8479\n",
            "Epoch 26/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.1829 - acc: 0.9343 - val_loss: 0.5235 - val_acc: 0.8617\n",
            "Epoch 27/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1350 - acc: 0.9494 - val_loss: 0.5051 - val_acc: 0.8646\n",
            "Epoch 28/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.1271 - acc: 0.9531 - val_loss: 0.5604 - val_acc: 0.8486\n",
            "Epoch 29/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1218 - acc: 0.9546 - val_loss: 0.6101 - val_acc: 0.8399\n",
            "Epoch 30/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1223 - acc: 0.9535 - val_loss: 0.5338 - val_acc: 0.8608\n",
            "Epoch 31/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.1180 - acc: 0.9553 - val_loss: 0.5476 - val_acc: 0.8622\n",
            "Epoch 32/100\n",
            "151/151 [==============================] - 10s 64ms/step - loss: 0.1110 - acc: 0.9583 - val_loss: 0.5354 - val_acc: 0.8688\n",
            "Epoch 33/100\n",
            "151/151 [==============================] - 10s 63ms/step - loss: 0.1143 - acc: 0.9589 - val_loss: 0.5803 - val_acc: 0.8548\n",
            "Epoch 34/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1048 - acc: 0.9607 - val_loss: 0.5800 - val_acc: 0.8568\n",
            "Epoch 35/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1030 - acc: 0.9616 - val_loss: 0.5819 - val_acc: 0.8561\n",
            "Epoch 36/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.1046 - acc: 0.9605 - val_loss: 0.6253 - val_acc: 0.8517\n",
            "Epoch 37/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1104 - acc: 0.9601 - val_loss: 0.6130 - val_acc: 0.8512\n",
            "Epoch 38/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0980 - acc: 0.9635 - val_loss: 0.6108 - val_acc: 0.8593\n",
            "Epoch 39/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0964 - acc: 0.9644 - val_loss: 0.6123 - val_acc: 0.8564\n",
            "Epoch 40/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0935 - acc: 0.9656 - val_loss: 0.6391 - val_acc: 0.8539\n",
            "Epoch 41/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0950 - acc: 0.9648 - val_loss: 0.6070 - val_acc: 0.8617\n",
            "Epoch 42/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0887 - acc: 0.9671 - val_loss: 0.6137 - val_acc: 0.8559\n",
            "Epoch 43/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0935 - acc: 0.9651 - val_loss: 0.6974 - val_acc: 0.8417\n",
            "Epoch 44/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0894 - acc: 0.9669 - val_loss: 0.6534 - val_acc: 0.8486\n",
            "Epoch 45/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0847 - acc: 0.9678 - val_loss: 0.6783 - val_acc: 0.8477\n",
            "Epoch 46/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0860 - acc: 0.9686 - val_loss: 0.6470 - val_acc: 0.8531\n",
            "Epoch 47/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0864 - acc: 0.9681 - val_loss: 0.7170 - val_acc: 0.8364\n",
            "Epoch 48/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0859 - acc: 0.9673 - val_loss: 0.7523 - val_acc: 0.8310\n",
            "Epoch 49/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0841 - acc: 0.9690 - val_loss: 0.6702 - val_acc: 0.8526\n",
            "Epoch 50/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0841 - acc: 0.9688 - val_loss: 0.6781 - val_acc: 0.8486\n",
            "Epoch 51/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0845 - acc: 0.9688 - val_loss: 0.6609 - val_acc: 0.8557\n",
            "Epoch 52/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0788 - acc: 0.9714 - val_loss: 0.6413 - val_acc: 0.8568\n",
            "Epoch 53/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0813 - acc: 0.9694 - val_loss: 0.7057 - val_acc: 0.8493\n",
            "Epoch 54/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0810 - acc: 0.9697 - val_loss: 0.6779 - val_acc: 0.8526\n",
            "Epoch 55/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0796 - acc: 0.9713 - val_loss: 0.7054 - val_acc: 0.8490\n",
            "Epoch 56/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0815 - acc: 0.9696 - val_loss: 0.6900 - val_acc: 0.8497\n",
            "Epoch 57/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0776 - acc: 0.9711 - val_loss: 0.7141 - val_acc: 0.8482\n",
            "Epoch 58/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0793 - acc: 0.9708 - val_loss: 0.6693 - val_acc: 0.8599\n",
            "Epoch 59/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0747 - acc: 0.9722 - val_loss: 0.6606 - val_acc: 0.8599\n",
            "Epoch 60/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0789 - acc: 0.9715 - val_loss: 0.6701 - val_acc: 0.8651\n",
            "Epoch 61/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0767 - acc: 0.9709 - val_loss: 0.6546 - val_acc: 0.8655\n",
            "Epoch 62/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0716 - acc: 0.9742 - val_loss: 0.6911 - val_acc: 0.8599\n",
            "Epoch 63/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0757 - acc: 0.9711 - val_loss: 0.6977 - val_acc: 0.8537\n",
            "Epoch 64/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0778 - acc: 0.9714 - val_loss: 0.7471 - val_acc: 0.8448\n",
            "Epoch 65/100\n",
            "151/151 [==============================] - 10s 64ms/step - loss: 0.0766 - acc: 0.9706 - val_loss: 0.7515 - val_acc: 0.8450\n",
            "Epoch 66/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0712 - acc: 0.9732 - val_loss: 0.7858 - val_acc: 0.8370\n",
            "Epoch 67/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0739 - acc: 0.9725 - val_loss: 0.7912 - val_acc: 0.8375\n",
            "Epoch 68/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0740 - acc: 0.9742 - val_loss: 0.7158 - val_acc: 0.8513\n",
            "Epoch 69/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0727 - acc: 0.9719 - val_loss: 0.6906 - val_acc: 0.8551\n",
            "Epoch 70/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0695 - acc: 0.9743 - val_loss: 0.6788 - val_acc: 0.8597\n",
            "Epoch 71/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0720 - acc: 0.9732 - val_loss: 0.7402 - val_acc: 0.8468\n",
            "Epoch 72/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0702 - acc: 0.9734 - val_loss: 0.6931 - val_acc: 0.8544\n",
            "Epoch 73/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0687 - acc: 0.9751 - val_loss: 0.7581 - val_acc: 0.8446\n",
            "Epoch 74/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0696 - acc: 0.9735 - val_loss: 0.7666 - val_acc: 0.8383\n",
            "Epoch 75/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0749 - acc: 0.9720 - val_loss: 0.7101 - val_acc: 0.8564\n",
            "Epoch 76/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0692 - acc: 0.9751 - val_loss: 0.7419 - val_acc: 0.8463\n",
            "Epoch 77/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0683 - acc: 0.9752 - val_loss: 0.7564 - val_acc: 0.8441\n",
            "Epoch 78/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0683 - acc: 0.9747 - val_loss: 0.7776 - val_acc: 0.8395\n",
            "Epoch 79/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0690 - acc: 0.9749 - val_loss: 0.7534 - val_acc: 0.8502\n",
            "Epoch 80/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0664 - acc: 0.9753 - val_loss: 0.6949 - val_acc: 0.8677\n",
            "Epoch 81/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0850 - acc: 0.9693 - val_loss: 0.7694 - val_acc: 0.8470\n",
            "Epoch 82/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0866 - acc: 0.9686 - val_loss: 0.7401 - val_acc: 0.8517\n",
            "Epoch 83/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.1078 - acc: 0.9633 - val_loss: 0.7244 - val_acc: 0.8582\n",
            "Epoch 84/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0694 - acc: 0.9743 - val_loss: 0.7519 - val_acc: 0.8541\n",
            "Epoch 85/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0694 - acc: 0.9746 - val_loss: 0.6913 - val_acc: 0.8604\n",
            "Epoch 86/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0671 - acc: 0.9745 - val_loss: 0.7813 - val_acc: 0.8466\n",
            "Epoch 87/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0664 - acc: 0.9761 - val_loss: 0.7478 - val_acc: 0.8515\n",
            "Epoch 88/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0675 - acc: 0.9747 - val_loss: 0.7675 - val_acc: 0.8499\n",
            "Epoch 89/100\n",
            "151/151 [==============================] - 9s 61ms/step - loss: 0.0656 - acc: 0.9768 - val_loss: 0.7676 - val_acc: 0.8519\n",
            "Epoch 90/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0651 - acc: 0.9750 - val_loss: 0.7584 - val_acc: 0.8548\n",
            "Epoch 91/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0675 - acc: 0.9756 - val_loss: 0.7193 - val_acc: 0.8573\n",
            "Epoch 92/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0661 - acc: 0.9756 - val_loss: 0.7496 - val_acc: 0.8542\n",
            "Epoch 93/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0639 - acc: 0.9767 - val_loss: 0.7322 - val_acc: 0.8544\n",
            "Epoch 94/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0654 - acc: 0.9765 - val_loss: 0.7028 - val_acc: 0.8630\n",
            "Epoch 95/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0641 - acc: 0.9755 - val_loss: 0.8113 - val_acc: 0.8433\n",
            "Epoch 96/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0671 - acc: 0.9741 - val_loss: 0.7614 - val_acc: 0.8470\n",
            "Epoch 97/100\n",
            "151/151 [==============================] - 9s 63ms/step - loss: 0.0671 - acc: 0.9760 - val_loss: 0.7576 - val_acc: 0.8570\n",
            "Epoch 98/100\n",
            "151/151 [==============================] - 10s 63ms/step - loss: 0.0647 - acc: 0.9763 - val_loss: 0.7574 - val_acc: 0.8542\n",
            "Epoch 99/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0675 - acc: 0.9758 - val_loss: 0.7768 - val_acc: 0.8459\n",
            "Epoch 100/100\n",
            "151/151 [==============================] - 9s 62ms/step - loss: 0.0637 - acc: 0.9760 - val_loss: 0.7797 - val_acc: 0.8493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlFC-t55TVCj",
        "colab_type": "text"
      },
      "source": [
        "### Głęboki LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuMS6ZemRqfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Yi_3jDLFYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}